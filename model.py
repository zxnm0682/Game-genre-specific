import os
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, backend as K
from typing import Tuple, List, Dict

# dataloader.pyì™€ visualize.pyì—ì„œ ì •ì˜í•œ ìƒìˆ˜ ë° í•¨ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.
from dataloader import DataManager, create_dataset, GENRE_LIST, TARGET_SIZE

# --- [1] ë‹¤ì¤‘ ë ˆì´ë¸” ì»¤ìŠ¤í…€ ì§€í‘œ ---

def exact_match_ratio(y_true, y_pred):
    """ëª¨ë“  ì¥ë¥´ë¥¼ ì™„ë²½í•˜ê²Œ ë§ì¶˜ ë¹„ìœ¨ (Strict Accuracy)"""
    threshold = 0.5
    y_pred_bin = K.cast(K.greater(y_pred, threshold), 'float32')
    match = K.all(K.equal(y_true, y_pred_bin), axis=1)
    return K.mean(match)

def micro_f1_score(y_true, y_pred):
    """ì „ì²´ ë ˆì´ë¸” í•©ì‚° ê¸°ì¤€ F1-Score (Microí‰ê· )"""
    threshold = 0.5
    y_pred_bin = K.cast(K.greater(y_pred, threshold), 'float32')
    
    tp = K.sum(y_true * y_pred_bin)
    fp = K.sum((1 - y_true) * y_pred_bin)
    fn = K.sum(y_true * (1 - y_pred_bin))
    
    precision = tp / (tp + fp + K.epsilon())
    recall = tp / (tp + fn + K.epsilon())
    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))

def jaccard_similarity(y_true, y_pred):
    """ì‹¤ì œ ë ˆì´ë¸”ê³¼ ì˜ˆì¸¡ ë ˆì´ë¸” ê°„ì˜ êµì§‘í•©/í•©ì§‘í•© ë¹„ìœ¨ (Intersection over Union)"""
    threshold = 0.5
    y_pred_bin = K.cast(K.greater(y_pred, threshold), 'float32')
    
    intersection = K.sum(y_true * y_pred_bin, axis=1)
    union = K.sum(K.clip(y_true + y_pred_bin, 0, 1), axis=1)
    return K.mean(intersection / (union + K.epsilon()))

# --- [2] ëª¨ë¸ êµ¬ì¡° ì •ì˜ (VGG-Style CNN) ---



def build_cnn_model(input_shape: Tuple[int, int, int], num_genres: int):
    """ë‹¤ì¤‘ ë ˆì´ë¸” ë¶„ë¥˜ë¥¼ ìœ„í•œ CNN ëª¨ë¸ ìƒì„± ë° ì»´íŒŒì¼"""
    model = keras.Sequential([
        layers.Input(shape=input_shape),

        # Block 1
        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.MaxPooling2D((2, 2)),
        layers.Dropout(0.2),

        # Block 2
        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.MaxPooling2D((2, 2)),
        layers.Dropout(0.2),

        # Block 3
        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),
        layers.BatchNormalization(),
        
        # Classifier
        layers.GlobalAveragePooling2D(),
        layers.Dropout(0.3),
        layers.Dense(512, activation='relu'),
        # ë‹¤ì¤‘ ë ˆì´ë¸”ì„ ìœ„í•´ sigmoid ì‚¬ìš©
        layers.Dense(num_genres, activation='sigmoid') 
    ])

    model.compile(
        optimizer='adam',
        loss='binary_crossentropy', # Multi-label í•µì‹¬ ì„¤ì •
        metrics=[
            'accuracy',
            exact_match_ratio,
            micro_f1_score,
            jaccard_similarity,
            tf.keras.metrics.Precision(name='precision'),
            tf.keras.metrics.Recall(name='recall')
        ]
    )
    return model

# --- [3] í•™ìŠµ ì‹¤í–‰ í•¨ìˆ˜ ---

def run_training(epochs: int, batch_size: int, samples_per_genre: int, base_dir: str):
    """í•™ìŠµ ë°ì´í„° ìƒ˜í”Œë§ ë° ëª¨ë¸ í›ˆë ¨ ë£¨í”„ ì‹¤í–‰"""
    
    # 1. ë°ì´í„° ë§¤ë‹ˆì € ì´ˆê¸°í™” (í›ˆë ¨ìš©)
    train_manager = DataManager(
        csv_path=os.path.join(base_dir, 'training_tag_vectors.csv'),
        data_root_dir=os.path.join(base_dir, 'Processed_224x224')
    )
    
    # 2. ê²€ì¦ ë°ì´í„°ì…‹ ì¤€ë¹„ (ê²€ì¦ì€ í•œ ë²ˆë§Œ ë¡œë“œí•˜ì—¬ ê³ ì • ì‚¬ìš©)
    val_manager = DataManager(
        csv_path=os.path.join(base_dir, 'validation_tag_vectors.csv'),
        data_root_dir=os.path.join(base_dir, 'Validation_Set')
    )
    v_paths, v_tags = val_manager.get_dataset_lists() # ëª¨ë“  ë°ì´í„° ë¡œë“œ
    val_dataset = create_dataset(v_paths, v_tags, batch_size, is_training=False)
    
    # 3. ëª¨ë¸ êµ¬ì¶•
    model = build_cnn_model(input_shape=(*TARGET_SIZE, 3), num_genres=len(GENRE_LIST))
    
    # í•™ìŠµ ê²°ê³¼ ê¸°ë¡ìš©
    final_history = {
        'loss': [], 'val_loss': [], 
        'accuracy': [], 'val_accuracy': [],
        'micro_f1_score': [], 'val_micro_f1_score': []
    }

    print(f"\nğŸš€ í•™ìŠµ ì‹œì‘ (ì´ {epochs} Epochs, ì—í­ë‹¹ ì¥ë¥´ë³„ {samples_per_genre}ê°œ ìƒ˜í”Œë§)")
    
    for epoch in range(epochs):
        print(f"\n[Epoch {epoch+1}/{epochs}]")
        
        # [í•µì‹¬] ë§¤ ì—í­ë§ˆë‹¤ ì¥ë¥´ë³„ë¡œ ë™ì¼í•œ ìˆ˜ë§Œí¼ ë¬´ì‘ìœ„ ìƒ˜í”Œë§ (ë°ì´í„° ê· í˜•í™”)
        t_paths, t_tags = train_manager.get_dataset_lists(samples_per_genre=samples_per_genre)
        train_dataset = create_dataset(t_paths, t_tags, batch_size, is_training=True)
        
        # í›ˆë ¨ ìˆ˜í–‰ (1ì—í­ì”©)
        history = model.fit(
            train_dataset,
            validation_data=val_dataset,
            epochs=1,
            verbose=1
        )
        
        # ê¸°ë¡ ì—…ë°ì´íŠ¸
        for key in final_history.keys():
            if key in history.history:
                final_history[key].append(history.history[key][0])

    print("\nâœ… ëª¨ë“  í•™ìŠµ ê³¼ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    return model, final_history, val_dataset