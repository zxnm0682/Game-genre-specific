import os
import random
import shutil
import glob
import pandas as pd
from typing import List, Dict, Set

# --- [1] í†µí•© ì„¤ì •ê°’ ---
BASE_DIR = r'F:\ML\dataset'
SOURCE_DIR_NAME = 'Processed_224x224'    # ì›ë³¸(í•™ìŠµìš©) ì´ë¯¸ì§€ í´ë”
VALIDATION_DIR_NAME = 'Validation_Set'   # ê²€ì¦ìš© ì´ë¯¸ì§€ ì €ì¥ í´ë”
FILES_TO_MOVE_PER_GENRE = 3000           # ì¥ë¥´ë³„ ì¶”ì¶œí•  ì´ë¯¸ì§€ ìˆ˜

FINAL_CSV_FILENAME = 'final_unique_tag_vectors.csv'     # ì „ì²´ í†µí•© CSV
VALIDATION_CSV_FILENAME = 'validation_tag_vectors.csv'  # ê²€ì¦ì…‹ ì „ìš© CSV

TARGET_TAGS = [
    "Adventure", "Action", "RPG", "Strategy", "Simulation", "Sports", "Racing", 
    "Puzzle", "Sandbox", "Shooter", "Survival"
]

# --- [2] í—¬í¼ í•¨ìˆ˜ (ìœ í‹¸ë¦¬í‹°) ---

def get_file_count_and_sort_tags(source_base_dir: str) -> List[str]:
    """ì¥ë¥´ë³„ íŒŒì¼ ìˆ˜ë¥¼ í™•ì¸í•˜ê³  ë°ì´í„°ê°€ ì ì€ ìˆœìœ¼ë¡œ ì •ë ¬í•©ë‹ˆë‹¤."""
    tag_counts = {}
    for tag in TARGET_TAGS:
        path = os.path.join(source_base_dir, tag)
        if os.path.isdir(path):
            tag_counts[tag] = len(glob.glob(os.path.join(path, '*.[pj][np]g')))
        else:
            tag_counts[tag] = 0
    
    sorted_tags = sorted(tag_counts.keys(), key=lambda t: tag_counts[t])
    print("ğŸ“Š ì¥ë¥´ë³„ ë°ì´í„° í˜„í™© (ì˜¤ë¦„ì°¨ìˆœ):")
    for tag in sorted_tags:
        print(f"  - {tag}: {tag_counts[tag]}ê°œ")
    return sorted_tags

def remove_duplicates_from_all_train_dirs(filename: str):
    """ê²€ì¦ì…‹ìœ¼ë¡œ ë½‘íŒ íŒŒì¼ì´ ë‹¤ë¥¸ í•™ìŠµìš© ì¥ë¥´ í´ë”ì— ë‚¨ì•„ìˆì§€ ì•Šë„ë¡ ì œê±°í•©ë‹ˆë‹¤."""
    source_base_dir = os.path.join(BASE_DIR, SOURCE_DIR_NAME)
    for tag in TARGET_TAGS:
        file_path = os.path.join(source_base_dir, tag, filename)
        if os.path.exists(file_path):
            try:
                os.remove(file_path)
            except Exception as e:
                print(f"  [ì˜¤ë¥˜] ì¤‘ë³µ ì œê±° ì‹¤íŒ¨ ({filename}): {e}")

# --- [3] í•µì‹¬ ê¸°ëŠ¥ 1: ì´ë¯¸ì§€ ë¶„ë¦¬ (Validation Set êµ¬ì¶•) ---

def run_data_split():
    """í•™ìŠµ í´ë”ì—ì„œ ì´ë¯¸ì§€ë¥¼ ëœë¤ ì¶”ì¶œí•˜ì—¬ ê²€ì¦ í´ë”ë¡œ ì´ë™ì‹œí‚µë‹ˆë‹¤."""
    source_base_dir = os.path.join(BASE_DIR, SOURCE_DIR_NAME)
    validation_base_dir = os.path.join(BASE_DIR, VALIDATION_DIR_NAME)
    os.makedirs(validation_base_dir, exist_ok=True)

    sorted_tags = get_file_count_and_sort_tags(source_base_dir)
    global_validation_filenames: Set[str] = set()

    # ê¸°ì¡´ ê²€ì¦ í´ë”ì— íŒŒì¼ì´ ìˆë‹¤ë©´ ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•´ ì¶”ì  ì…‹ì— ì¶”ê°€
    for tag in TARGET_TAGS:
        v_path = os.path.join(validation_base_dir, tag)
        if os.path.isdir(v_path):
            files = glob.glob(os.path.join(v_path, '*.[pj][np]g'))
            global_validation_filenames.update({os.path.basename(f) for f in files})

    print("-" * 50)
    print(f"ğŸš€ ê²€ì¦ì…‹ ë¶„ë¦¬ ì‹œì‘ (ëª©í‘œ: ì¥ë¥´ë³„ {FILES_TO_MOVE_PER_GENRE}ê°œ)")
    
    for tag in sorted_tags:
        s_dir = os.path.join(source_base_dir, tag)
        v_dir = os.path.join(validation_base_dir, tag)
        os.makedirs(v_dir, exist_ok=True)

        current_v_count = len(glob.glob(os.path.join(v_dir, '*.[pj][np]g')))
        if current_v_count >= FILES_TO_MOVE_PER_GENRE:
            print(f"[{tag}] âœ… ì´ë¯¸ ëª©í‘œì¹˜ ë‹¬ì„±. ìŠ¤í‚µ.")
            continue

        needed = FILES_TO_MOVE_PER_GENRE - current_v_count
        all_files = glob.glob(os.path.join(s_dir, '*.[pj][np]g'))
        
        # ë‹¤ë¥¸ ì¥ë¥´ì—ì„œ ì´ë¯¸ ë½‘íŒ íŒŒì¼ ì œì™¸
        candidates = [f for f in all_files if os.path.basename(f) not in global_validation_filenames]

        if not candidates:
            print(f"[{tag}] âš ï¸ ì´ë™í•  ìˆ˜ ìˆëŠ” ê³ ìœ  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            continue

        selected = random.sample(candidates, min(len(candidates), needed))
        
        moved_count = 0
        for s_path in selected:
            fname = os.path.basename(s_path)
            d_path = os.path.join(v_dir, fname)
            try:
                # 1. íŒŒì¼ ì´ë™
                shutil.move(s_path, d_path)
                # 2. ë‹¤ë¥¸ í•™ìŠµ í´ë” ë‚´ ë™ì¼ íŒŒì¼ ì‚­ì œ (ë°ì´í„° ì˜¤ì—¼ ë°©ì§€)
                remove_duplicates_from_all_train_dirs(fname)
                global_validation_filenames.add(fname)
                moved_count += 1
            except Exception as e:
                print(f"  [ì˜¤ë¥˜] {fname} ì´ë™ ì‹¤íŒ¨: {e}")

        print(f"[{tag}] ì™„ë£Œ: {current_v_count + moved_count}ê°œ í™•ë³´ (ì´ë²ˆì— {moved_count}ê°œ ì´ë™)")

    print(f"âœ… ê²€ì¦ì…‹ ì´ë¯¸ì§€ ë¶„ë¦¬ ì™„ë£Œ. ì´ ê³ ìœ  ì´ë¯¸ì§€: {len(global_validation_filenames)}ì¥")

# --- [4] í•µì‹¬ ê¸°ëŠ¥ 2: ê²€ì¦ì…‹ìš© CSV ìƒì„± ---

def run_create_validation_csv():
    """ì‹¤ì œ ê²€ì¦ í´ë”ì— ìˆëŠ” íŒŒì¼ë“¤ë§Œ í•„í„°ë§í•˜ì—¬ ì „ìš© CSVë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    print("-" * 50)
    print("ğŸ“Š ê²€ì¦ì…‹ ì „ìš© CSV ìƒì„± ì‹œì‘...")
    
    v_base_dir = os.path.join(BASE_DIR, VALIDATION_DIR_NAME)
    source_csv_path = os.path.join(BASE_DIR, FINAL_CSV_FILENAME)
    output_csv_path = os.path.join(BASE_DIR, VALIDATION_CSV_FILENAME)

    # 1. ì‹¤ì œ í´ë” ë‚´ íŒŒì¼ëª… ìˆ˜ì§‘
    search_pattern = os.path.join(v_base_dir, '**', '*.[pj][np]g')
    v_files = glob.glob(search_pattern, recursive=True)
    v_filenames = {os.path.basename(f) for f in v_files}

    if not v_filenames:
        print("âŒ ì˜¤ë¥˜: ê²€ì¦ í´ë”ì— ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤."); return

    # 2. ì›ë³¸ CSV ë¡œë“œ ë° í•„í„°ë§
    try:
        df_full = pd.read_csv(source_csv_path)
        df_val = df_full[df_full['filename'].isin(v_filenames)]
        
        df_val.to_csv(output_csv_path, index=False, encoding='utf-8')
        print(f"âœ… ê²€ì¦ìš© CSV ì €ì¥ ì™„ë£Œ: {output_csv_path}")
        print(f"   (ë§¤ì¹­ëœ í–‰ ìˆ˜: {len(df_val)} / ì‹¤ì œ íŒŒì¼ ìˆ˜: {len(v_filenames)})")
    except Exception as e:
        print(f"âŒ CSV ìƒì„± ì‹¤íŒ¨: {e}")

# --- [5] ì‹¤í–‰ ì œì–´ ---

if __name__ == "__main__":
    # 1. í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¬¼ë¦¬ì  ë¶„ë¦¬ ì‹¤í–‰
    run_data_split()
    
    # 2. ë¶„ë¦¬ëœ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê²€ì¦ìš© CSV ìƒì„±
    run_create_validation_csv()