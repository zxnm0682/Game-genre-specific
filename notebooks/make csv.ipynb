{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "811a1554-0efc-431a-ba2f-500f554275f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” ì´ 11ê°œì˜ CSV íŒŒì¼ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. í†µí•©ì„ ì‹œì‘í•©ë‹ˆë‹¤...\n",
      "   -> íŒŒì¼ ë¡œë“œ ì™„ë£Œ: screenshot_tag_vectors_Action.csv (315200í–‰)\n",
      "   -> íŒŒì¼ ë¡œë“œ ì™„ë£Œ: screenshot_tag_vectors_Adventure.csv (291501í–‰)\n",
      "   -> íŒŒì¼ ë¡œë“œ ì™„ë£Œ: screenshot_tag_vectors_Puzzle.csv (113452í–‰)\n",
      "   -> íŒŒì¼ ë¡œë“œ ì™„ë£Œ: screenshot_tag_vectors_Racing.csv (29494í–‰)\n",
      "   -> íŒŒì¼ ë¡œë“œ ì™„ë£Œ: screenshot_tag_vectors_RPG.csv (136614í–‰)\n",
      "   -> íŒŒì¼ ë¡œë“œ ì™„ë£Œ: screenshot_tag_vectors_Sandbox.csv (47764í–‰)\n",
      "   -> íŒŒì¼ ë¡œë“œ ì™„ë£Œ: screenshot_tag_vectors_Shooter.csv (78453í–‰)\n",
      "   -> íŒŒì¼ ë¡œë“œ ì™„ë£Œ: screenshot_tag_vectors_Simulation.csv (155824í–‰)\n",
      "   -> íŒŒì¼ ë¡œë“œ ì™„ë£Œ: screenshot_tag_vectors_Sports.csv (34998í–‰)\n",
      "   -> íŒŒì¼ ë¡œë“œ ì™„ë£Œ: screenshot_tag_vectors_Strategy.csv (148014í–‰)\n",
      "   -> íŒŒì¼ ë¡œë“œ ì™„ë£Œ: screenshot_tag_vectors_Survival.csv (60314í–‰)\n",
      "--------------------------------------------------\n",
      "ì´ í†µí•©ëœ í–‰ ìˆ˜ (ì¤‘ë³µ í¬í•¨): 1411628í–‰\n",
      "âœ… ì¤‘ë³µ ì œê±° ì™„ë£Œ. 781602ê°œì˜ ì¤‘ë³µëœ ìŠ¤í¬ë¦°ìƒ·ì´ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "--------------------------------------------------\n",
      "ìµœì¢… í†µí•©ëœ ê³ ìœ  ìŠ¤í¬ë¦°ìƒ· ìˆ˜: 630026í–‰\n",
      "ìµœì¢… íŒŒì¼ ì €ì¥ ê²½ë¡œ: F:\\ML\\dataset\\final_unique_tag_vectors.csv\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# ğŸ’¡ ì´ ì„¤ì •ê°’ë“¤ì„ ì‚¬ìš©ìì˜ í™˜ê²½ì— ë§ê²Œ ë°˜ë“œì‹œ ìˆ˜ì •í•˜ì„¸ìš”!\n",
    "# -------------------------------------------------------------\n",
    "BASE_DOWNLOAD_DIR = r'F:\\ML\\dataset'      # ğŸ‘ˆ ì¥ë¥´ë³„ CSV íŒŒì¼ë“¤ì´ ì €ì¥ëœ ê¸°ë³¸ ê²½ë¡œ\n",
    "CSV_FILENAME_PREFIX = 'screenshot_tag_vectors_' # ğŸ‘ˆ ì¥ë¥´ë³„ CSV íŒŒì¼ëª…ì˜ ê³µí†µ ì‹œì‘ ë¶€ë¶„\n",
    "FINAL_CSV_FILENAME = 'final_unique_tag_vectors.csv' # ğŸ‘ˆ ìµœì¢… í†µí•© íŒŒì¼ëª…\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "def merge_csv_files():\n",
    "    \"\"\"\n",
    "    ì§€ì •ëœ ê²½ë¡œì˜ ëª¨ë“  ì¥ë¥´ë³„ CSV íŒŒì¼ì„ í†µí•©í•˜ê³  íŒŒì¼ëª… ì¤‘ë³µì„ ì œê±°í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    search_path = os.path.join(BASE_DOWNLOAD_DIR, f'{CSV_FILENAME_PREFIX}*.csv')\n",
    "    \n",
    "    # 1. íŒŒì¼ëª… íŒ¨í„´ì— ë§ëŠ” ëª¨ë“  CSV íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°\n",
    "    csv_files = glob.glob(search_path)\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(f\"âŒ ì˜¤ë¥˜: ê²½ë¡œ {BASE_DOWNLOAD_DIR}ì—ì„œ '{CSV_FILENAME_PREFIX}*.csv' íŒ¨í„´ì— ë§ëŠ” CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "\n",
    "    print(f\"ğŸ” ì´ {len(csv_files)}ê°œì˜ CSV íŒŒì¼ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. í†µí•©ì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "    \n",
    "    all_data_frames = []\n",
    "    \n",
    "    # 2. ëª¨ë“  CSV íŒŒì¼ì„ ì½ì–´ DataFrame ë¦¬ìŠ¤íŠ¸ì— ì €ì¥\n",
    "    for f in csv_files:\n",
    "        try:\n",
    "            df = pd.read_csv(f)\n",
    "            all_data_frames.append(df)\n",
    "            print(f\"   -> íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {os.path.basename(f)} ({len(df)}í–‰)\")\n",
    "        except Exception as e:\n",
    "            print(f\"   -> âš ï¸ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ ({os.path.basename(f)}): {e}\")\n",
    "\n",
    "    if not all_data_frames:\n",
    "        print(\"âŒ ì˜¤ë¥˜: ì„±ê³µì ìœ¼ë¡œ ë¡œë“œëœ CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "\n",
    "    # 3. ëª¨ë“  DataFrameì„ í•˜ë‚˜ë¡œ í†µí•©\n",
    "    df_combined = pd.concat(all_data_frames, ignore_index=True)\n",
    "    total_rows_before = len(df_combined)\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"ì´ í†µí•©ëœ í–‰ ìˆ˜ (ì¤‘ë³µ í¬í•¨): {total_rows_before}í–‰\")\n",
    "\n",
    "    # 4. 'filename' ì—´ì„ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±° (ì²« ë²ˆì§¸ í–‰ ìœ ì§€)\n",
    "    # í•œ ìŠ¤í¬ë¦°ìƒ·ì´ ì—¬ëŸ¬ ì¥ë¥´ì— ë§¤ì¹­ë˜ì–´ CSVì— ì¤‘ë³µ ê¸°ë¡ë˜ì—ˆì„ ê²½ìš°,\n",
    "    # ì¤‘ë³µëœ íŒŒì¼ëª…ì€ í•˜ë‚˜ë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ëŠ” ì œê±°í•©ë‹ˆë‹¤.\n",
    "    df_unique = df_combined.drop_duplicates(subset=['filename'], keep='first')\n",
    "    total_rows_after = len(df_unique)\n",
    "    \n",
    "    duplicates_removed = total_rows_before - total_rows_after\n",
    "    \n",
    "    # 5. ìµœì¢… íŒŒì¼ ì €ì¥\n",
    "    final_output_path = os.path.join(BASE_DOWNLOAD_DIR, FINAL_CSV_FILENAME)\n",
    "    df_unique.to_csv(final_output_path, index=False, encoding='utf-8')\n",
    "\n",
    "    print(f\"âœ… ì¤‘ë³µ ì œê±° ì™„ë£Œ. {duplicates_removed}ê°œì˜ ì¤‘ë³µëœ ìŠ¤í¬ë¦°ìƒ·ì´ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"ìµœì¢… í†µí•©ëœ ê³ ìœ  ìŠ¤í¬ë¦°ìƒ· ìˆ˜: {total_rows_after}í–‰\")\n",
    "    print(f\"ìµœì¢… íŒŒì¼ ì €ì¥ ê²½ë¡œ: {final_output_path}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# í•¨ìˆ˜ ì‹¤í–‰\n",
    "merge_csv_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ded69530-2420-4ff2-9578-0a0779b1c886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Validation_Set í´ë”ì—ì„œ íŒŒì¼ëª… ëª©ë¡ì„ ìˆ˜ì§‘ ì¤‘...\n",
      "âœ… Validation_Setì—ì„œ 33000ê°œì˜ ê³ ìœ  íŒŒì¼ëª…ì„ í™•ë³´í–ˆìŠµë‹ˆë‹¤.\n",
      "ğŸ“Š ì›ë³¸ CSV íŒŒì¼ ë¡œë“œ ì™„ë£Œ. ì´ 630026í–‰.\n",
      "--------------------------------------------------\n",
      "ğŸ‰ ê²€ì¦ìš© CSV ìƒì„± ì™„ë£Œ. ì´ 33000í–‰ ì €ì¥.\n",
      "ì €ì¥ ê²½ë¡œ: F:\\ML\\dataset\\validation_tag_vectors.csv\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "from typing import Set\n",
    "\n",
    "# ğŸ’¡ ì´ ì„¤ì •ê°’ë“¤ì„ í™˜ê²½ì— ë§ê²Œ ë°˜ë“œì‹œ ìˆ˜ì •í•˜ì„¸ìš”!\n",
    "# -------------------------------------------------------------\n",
    "BASE_DIR = r'F:\\ML\\dataset'                 # ğŸ‘ˆ ëª¨ë“  ë°ì´í„° í´ë”ì˜ ìƒìœ„ ê²½ë¡œ\n",
    "FINAL_CSV_FILENAME = 'final_unique_tag_vectors.csv' # ğŸ‘ˆ í†µí•©ëœ ì›ë³¸ CSV íŒŒì¼ëª…\n",
    "VALIDATION_DIR_NAME = 'Validation_Set' # ğŸ‘ˆ ê²€ì¦ ì´ë¯¸ì§€ê°€ ì €ì¥ëœ í´ë”ëª…\n",
    "VALIDATION_CSV_FILENAME = 'validation_tag_vectors.csv' # ğŸ‘ˆ ìµœì¢… ì €ì¥ë  ê²€ì¦ìš© CSV íŒŒì¼ëª…\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "\n",
    "def create_validation_csv():\n",
    "    \"\"\"\n",
    "    Validation_Set í´ë” ë‚´ì˜ ì´ë¯¸ì§€ íŒŒì¼ëª…ë§Œ í•„í„°ë§í•˜ì—¬ ê²€ì¦ìš© CSVë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    \n",
    "    validation_base_dir = os.path.join(BASE_DIR, VALIDATION_DIR_NAME)\n",
    "    source_csv_path = os.path.join(BASE_DIR, FINAL_CSV_FILENAME)\n",
    "    output_csv_path = os.path.join(BASE_DIR, VALIDATION_CSV_FILENAME)\n",
    "    \n",
    "    # 1. Validation_Set í´ë” ë‚´ ëª¨ë“  íŒŒì¼ëª…(filename) í™•ë³´\n",
    "    validation_filenames: Set[str] = set()\n",
    "    \n",
    "    print(f\"ğŸ” {VALIDATION_DIR_NAME} í´ë”ì—ì„œ íŒŒì¼ëª… ëª©ë¡ì„ ìˆ˜ì§‘ ì¤‘...\")\n",
    "    \n",
    "    # Validation_Set ì•„ë˜ì˜ ëª¨ë“  ì¥ë¥´ í´ë”ë¥¼ ìˆœíšŒ\n",
    "    # glob.glob ì¬ê·€ì  ê²€ìƒ‰ (**/*)ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  í•˜ìœ„ í´ë”ì˜ íŒŒì¼ì„ ì°¾ìŠµë‹ˆë‹¤.\n",
    "    # [pj]gëŠ” .jpgì™€ .png íŒŒì¼ì„ ì˜ë¯¸\n",
    "    search_pattern = os.path.join(validation_base_dir, '**', '*.[pj][np]g')\n",
    "    \n",
    "    # recursive=Trueë¥¼ ì‚¬ìš©í•˜ì—¬ í•˜ìœ„ í´ë”ê¹Œì§€ íƒìƒ‰\n",
    "    all_validation_files = glob.glob(search_pattern, recursive=True)\n",
    "\n",
    "    for file_path in all_validation_files:\n",
    "        validation_filenames.add(os.path.basename(file_path))\n",
    "\n",
    "    if not validation_filenames:\n",
    "        print(f\"âŒ ì˜¤ë¥˜: {VALIDATION_DIR_NAME} í´ë”ì—ì„œ ì´ë¯¸ì§€ íŒŒì¼ì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "        return\n",
    "\n",
    "    print(f\"âœ… {VALIDATION_DIR_NAME}ì—ì„œ {len(validation_filenames)}ê°œì˜ ê³ ìœ  íŒŒì¼ëª…ì„ í™•ë³´í–ˆìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    # 2. ì›ë³¸ í†µí•© CSV íŒŒì¼ ë¡œë“œ\n",
    "    try:\n",
    "        df_full = pd.read_csv(source_csv_path)\n",
    "        print(f\"ğŸ“Š ì›ë³¸ CSV íŒŒì¼ ë¡œë“œ ì™„ë£Œ. ì´ {len(df_full)}í–‰.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ ì˜¤ë¥˜: ì›ë³¸ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”: {source_csv_path}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì˜¤ë¥˜: CSV íŒŒì¼ ë¡œë“œ ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}\")\n",
    "        return\n",
    "\n",
    "    # 3. 'filename' ì—´ì„ ê¸°ì¤€ìœ¼ë¡œ í•„í„°ë§\n",
    "    # df_full['filename'].isin(validation_filenames)ì€ filenameì´ setì— ìˆëŠ”ì§€ True/Falseë¡œ ë°˜í™˜\n",
    "    df_validation = df_full[df_full['filename'].isin(validation_filenames)]\n",
    "\n",
    "    # 4. ê²°ê³¼ ì¶œë ¥ ë° ì €ì¥\n",
    "    if len(df_validation) != len(validation_filenames):\n",
    "        print(f\"âš ï¸ ê²½ê³ : CSVì—ì„œ {len(df_validation)}ê°œ, í´ë”ì—ì„œ {len(validation_filenames)}ê°œ ì¼ì¹˜. ë¶ˆì¼ì¹˜í•˜ëŠ” íŒŒì¼ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    df_validation.to_csv(output_csv_path, index=False, encoding='utf-8')\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"ğŸ‰ ê²€ì¦ìš© CSV ìƒì„± ì™„ë£Œ. ì´ {len(df_validation)}í–‰ ì €ì¥.\")\n",
    "    print(f\"ì €ì¥ ê²½ë¡œ: {output_csv_path}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "\n",
    "# í•¨ìˆ˜ ì‹¤í–‰\n",
    "create_validation_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb081da-bbae-451a-8fea-cc00945949af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import glob\n",
    "from typing import List, Dict, Set\n",
    "\n",
    "# ğŸ’¡ ì„¤ì •ê°’ë“¤ì„ í™˜ê²½ì— ë§ê²Œ ë°˜ë“œì‹œ ìˆ˜ì •í•˜ì„¸ìš”!\n",
    "# -------------------------------------------------------------\n",
    "BASE_DIR = r'F:\\ML\\dataset'                 # ğŸ‘ˆ ëª¨ë“  ë°ì´í„° í´ë”ì˜ ìƒìœ„ ê²½ë¡œ\n",
    "SOURCE_DIR_NAME = 'Processed_224x224'  # ğŸ‘ˆ ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€ê°€ ì €ì¥ëœ í´ë”ëª…\n",
    "VALIDATION_DIR_NAME = 'Validation_Set' # ğŸ‘ˆ ê²€ì¦ ë°ì´í„°ì…‹ì„ ì €ì¥í•  ìƒˆ í´ë”ëª…\n",
    "FILES_TO_MOVE_PER_GENRE = 2000         # ğŸ‘ˆ ì¥ë¥´ë³„ ëª©í‘œ íŒŒì¼ ìˆ˜\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "TARGET_TAGS: List[str] = [\n",
    "    \"Adventure\", \"Action\", \"RPG\", \"Strategy\", \"Simulation\", \"Sports\", \"Racing\", \n",
    "    \"Puzzle\", \"Sandbox\", \"Shooter\", \"Survival\", \"Fighting\", \"Music\"\n",
    "]\n",
    "\n",
    "def get_file_count_and_sort_tags(source_base_dir: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    ê° ì¥ë¥´ í´ë”ì˜ íŒŒì¼ ìˆ˜ë¥¼ ì„¸ì–´ ë°ì´í„°ê°€ ì ì€ ì¥ë¥´ë¶€í„° ì˜¤ë¦„ì°¨ìˆœìœ¼ë¡œ ì •ë ¬í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    tag_counts: Dict[str, int] = {}\n",
    "    print(\"ğŸ” ê° ì¥ë¥´ë³„ íŒŒì¼ ìˆ˜ë¥¼ í™•ì¸ ì¤‘...\")\n",
    "    \n",
    "    for tag in TARGET_TAGS:\n",
    "        source_genre_dir = os.path.join(source_base_dir, tag)\n",
    "        if os.path.isdir(source_genre_dir):\n",
    "            # ì´ë¯¸ì§€ íŒŒì¼ (jpg, png)ë§Œ ì¹´ìš´íŠ¸\n",
    "            file_paths = glob.glob(os.path.join(source_genre_dir, '*.[pj][np]g'))\n",
    "            tag_counts[tag] = len(file_paths)\n",
    "        else:\n",
    "            tag_counts[tag] = 0\n",
    "            \n",
    "    # íŒŒì¼ ìˆ˜ ì˜¤ë¦„ì°¨ìˆœìœ¼ë¡œ íƒœê·¸ ì •ë ¬\n",
    "    sorted_tags = sorted(tag_counts.keys(), key=lambda t: tag_counts[t])\n",
    "    \n",
    "    print(\"âœ… ë°ì´í„°ê°€ ì ì€ ì¥ë¥´ë¶€í„° ìˆœì„œëŒ€ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤:\")\n",
    "    for tag in sorted_tags:\n",
    "        print(f\"   - {tag}: {tag_counts[tag]}ê°œ\")\n",
    "        \n",
    "    return sorted_tags\n",
    "\n",
    "# ğŸ’¡ ìƒˆë¡œìš´ í—¬í¼ í•¨ìˆ˜: í•™ìŠµ ë°ì´í„°ì…‹ì—ì„œ ê²€ì¦ íŒŒì¼ê³¼ ì¤‘ë³µë˜ëŠ” ëª¨ë“  íŒŒì¼ ì‚­ì œ\n",
    "def remove_duplicates_from_all_train_dirs(\n",
    "    base_dir: str, \n",
    "    source_dir_name: str, \n",
    "    filename_to_remove: str, \n",
    "    target_tags: List[str]\n",
    "):\n",
    "    \"\"\"\n",
    "    ëª¨ë“  ì¥ë¥´ í´ë”ë¥¼ ìˆœíšŒí•˜ë©° ì§€ì •ëœ íŒŒì¼ëª…ê³¼ ë™ì¼í•œ íŒŒì¼ì„ ì‚­ì œí•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    source_base_dir = os.path.join(base_dir, source_dir_name)\n",
    "\n",
    "    for tag in target_tags:\n",
    "        train_genre_dir = os.path.join(source_base_dir, tag)\n",
    "        file_path = os.path.join(train_genre_dir, filename_to_remove)\n",
    "        \n",
    "        if os.path.exists(file_path):\n",
    "            try:\n",
    "                os.remove(file_path)\n",
    "                # print(f\"    [ì¤‘ë³µ ì œê±°] {tag} í´ë”ì—ì„œ {filename_to_remove} ì‚­ì œ ì™„ë£Œ.\")\n",
    "            except Exception as e:\n",
    "                print(f\"    [ì¤‘ë³µ ì œê±°] âŒ ì‚­ì œ ì˜¤ë¥˜ ({file_path}): {e}\")\n",
    "\n",
    "def create_unique_validation_set():\n",
    "    \"\"\"\n",
    "    ê° ì¥ë¥´ì—ì„œ 2000ê°œì˜ ê³ ìœ í•œ íŒŒì¼ì„ ëœë¤ ì„ íƒí•˜ì—¬ ê²€ì¦ ë°ì´í„°ì…‹ìœ¼ë¡œ ì´ë™ì‹œí‚µë‹ˆë‹¤.\n",
    "    ëª¨ë“  ê²€ì¦ í´ë”ë¥¼ í†µí‹€ì–´ íŒŒì¼ëª…ì´ ì¤‘ë³µë˜ì§€ ì•Šë„ë¡ ë³´ì¥í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    \n",
    "    source_base_dir = os.path.join(BASE_DIR, SOURCE_DIR_NAME)\n",
    "    validation_base_dir = os.path.join(BASE_DIR, VALIDATION_DIR_NAME)\n",
    "    os.makedirs(validation_base_dir, exist_ok=True)\n",
    "    \n",
    "    # ë°ì´í„°ê°€ ì ì€ ì¥ë¥´ë¶€í„° ì²˜ë¦¬í•˜ë„ë¡ íƒœê·¸ ìˆœì„œ ì •ë ¬\n",
    "    sorted_tags = get_file_count_and_sort_tags(source_base_dir)\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"ê²€ì¦ ë°ì´í„°ì…‹ ë¶„ë¦¬ ì‹œì‘. ì¥ë¥´ë³„ ëª©í‘œ: {FILES_TO_MOVE_PER_GENRE}ê°œ\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # 1. ì „ì—­ ì¶”ì : ì´ë¯¸ ê²€ì¦ ë°ì´í„°ì…‹ì— í¬í•¨ëœ ëª¨ë“  íŒŒì¼ëª… (ì¤‘ë³µ ë°©ì§€ìš©)\n",
    "    global_validation_filenames: Set[str] = set()\n",
    "    \n",
    "    # ê¸°ì¡´ì— ì´ë™ëœ íŒŒì¼ì´ ìˆë‹¤ë©´ ëª©ë¡ì— ì¶”ê°€í•˜ì—¬ ì¬ì‹¤í–‰ ì‹œ ì¤‘ë³µë˜ì§€ ì•Šë„ë¡ í•¨\n",
    "    for tag in TARGET_TAGS:\n",
    "        validation_genre_dir = os.path.join(validation_base_dir, tag)\n",
    "        if os.path.isdir(validation_genre_dir):\n",
    "            moved_files = glob.glob(os.path.join(validation_genre_dir, '*.[pj][np]g'))\n",
    "            global_validation_filenames.update({os.path.basename(f) for f in moved_files})\n",
    "\n",
    "    total_moved = len(global_validation_filenames)\n",
    "    print(f\"ğŸš¨ í˜„ì¬ ê²€ì¦ ë°ì´í„°ì…‹ì— ì´ë¯¸ {total_moved}ê°œì˜ ê³ ìœ í•œ íŒŒì¼ì´ ìˆìŠµë‹ˆë‹¤. ì‘ì—…ì„ ì´ì–´ê°‘ë‹ˆë‹¤.\")\n",
    "\n",
    "    # 2. ì •ë ¬ëœ ìˆœì„œëŒ€ë¡œ ì¥ë¥´ë³„ íŒŒì¼ ì´ë™ ì‹œì‘\n",
    "    for tag in sorted_tags:\n",
    "        source_genre_dir = os.path.join(source_base_dir, tag)\n",
    "        validation_genre_dir = os.path.join(validation_base_dir, tag)\n",
    "        os.makedirs(validation_genre_dir, exist_ok=True)\n",
    "        \n",
    "        # 3. í˜„ì¬ ì¥ë¥´ í´ë”ì— ì´ë¯¸ ì´ë™ëœ íŒŒì¼ ìˆ˜ í™•ì¸\n",
    "        current_moved_in_genre = len(glob.glob(os.path.join(validation_genre_dir, '*.[pj][np]g')))\n",
    "        \n",
    "        if current_moved_in_genre >= FILES_TO_MOVE_PER_GENRE:\n",
    "            print(f\"[{tag}] âœ… ëª©í‘œ ìˆ˜ ({FILES_TO_MOVE_PER_GENRE}ê°œ) ë‹¬ì„± ì™„ë£Œ. ìŠ¤í‚µí•©ë‹ˆë‹¤.\")\n",
    "            continue\n",
    "\n",
    "        needed_to_move = FILES_TO_MOVE_PER_GENRE - current_moved_in_genre\n",
    "        \n",
    "        # 4. ì›ë³¸ í´ë”ì—ì„œ ì´ë™ ê°€ëŠ¥í•œ ëª¨ë“  íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°\n",
    "        all_source_files = glob.glob(os.path.join(source_genre_dir, '*.[pj][np]g'))\n",
    "        \n",
    "        # 5. ì „ì—­ ì¤‘ë³µì„ í™•ì¸í•˜ì—¬ ì´ë™ í›„ë³´ íŒŒì¼ ëª©ë¡ ìƒì„±\n",
    "        # (ì•„ì§ ê²€ì¦ ë°ì´í„°ì…‹ì˜ ì–´ë–¤ í´ë”ì—ë„ ë“¤ì–´ê°€ì§€ ì•Šì€ íŒŒì¼ë§Œ í›„ë³´ê°€ ë¨)\n",
    "        unique_candidates = [\n",
    "            f for f in all_source_files \n",
    "            if os.path.basename(f) not in global_validation_filenames\n",
    "        ]\n",
    "\n",
    "        # 6. ëœë¤ ì„ íƒ ë° ì´ë™\n",
    "        if not unique_candidates:\n",
    "            print(f\"[{tag}] âš ï¸ ì´ë™ ê°€ëŠ¥í•œ ê³ ìœ  íŒŒì¼ì´ ë¶€ì¡±í•˜ê±°ë‚˜ ë‚¨ì•„ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. (í˜„ì¬: {current_moved_in_genre}ê°œ)\")\n",
    "            continue\n",
    "            \n",
    "        if len(unique_candidates) < needed_to_move:\n",
    "            # ì›ë³¸ì— ë‚¨ì•„ìˆëŠ” ê³ ìœ  íŒŒì¼ì´ ë¶€ì¡±í•œ ê²½ìš°\n",
    "            selected_files = unique_candidates\n",
    "            print(f\"[{tag}] âš ï¸ íŒŒì¼ ë¶€ì¡±: {len(selected_files)}ê°œë§Œ ì´ë™í•©ë‹ˆë‹¤.\")\n",
    "        else:\n",
    "            # í•„ìš”í•œ íŒŒì¼ ìˆ˜ë§Œí¼ ëœë¤ ì„ íƒ\n",
    "            selected_files = random.sample(unique_candidates, needed_to_move)\n",
    "\n",
    "        moved_count = 0\n",
    "        \n",
    "        for source_path in selected_files:\n",
    "            filename = os.path.basename(source_path)\n",
    "            destination_path = os.path.join(validation_genre_dir, filename)\n",
    "            \n",
    "            try:\n",
    "                # íŒŒì¼ì„ ì›ë³¸ í´ë”ì—ì„œ ê²€ì¦ í´ë”ë¡œ ì´ë™ (ì›ë³¸ í´ë”ì—ì„œëŠ” ì‚­ì œë¨)\n",
    "                shutil.move(source_path, destination_path)\n",
    "                \n",
    "               # 2. ğŸ’¡ í•µì‹¬ ì¶”ê°€ ë¡œì§: ë‚˜ë¨¸ì§€ ëª¨ë“  ì¥ë¥´ í´ë”ì—ì„œ ë™ì¼ íŒŒì¼ëª…ì„ ì‚­ì œ\n",
    "                remove_duplicates_from_all_train_dirs(\n",
    "                    BASE_DIR, \n",
    "                    SOURCE_DIR_NAME, \n",
    "                    filename, \n",
    "                    TARGET_TAGS\n",
    "                )\n",
    "                \n",
    "                # 3. ì „ì—­ ì¶”ì  ì…‹ì— íŒŒì¼ëª… ì¶”ê°€ ë° ì¹´ìš´íŠ¸\n",
    "                global_validation_filenames.add(filename)\n",
    "                moved_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"[{tag}] âŒ íŒŒì¼ ì´ë™ ì˜¤ë¥˜ ({filename}): {e}\")\n",
    "\n",
    "        print(f\"[{tag}] ì´ë™ ì™„ë£Œ. ì´ {current_moved_in_genre + moved_count}ê°œ í™•ë³´ (ì´ë²ˆì— {moved_count}ê°œ ì´ë™).\")\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"ğŸ‰ ëª¨ë“  ì¥ë¥´ ë¶„ë¦¬ ì™„ë£Œ.\")\n",
    "    print(f\"ìµœì¢…ì ìœ¼ë¡œ ê²€ì¦ ë°ì´í„°ì…‹ì— í™•ë³´ëœ ê³ ìœ  ì´ë¯¸ì§€ ìˆ˜: {len(global_validation_filenames)}ì¥\")\n",
    "    print(f\"ê²€ì¦ ë°ì´í„°ì…‹ ê²½ë¡œ: {validation_base_dir}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# í•¨ìˆ˜ ì‹¤í–‰\n",
    "create_unique_validation_set()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
